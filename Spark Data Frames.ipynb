{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= spark.read.json(\"people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age: string, name: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#describe gives the statistic summary of the numeric column\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataschema= [StructField('age', IntegerType(), True) , StructField('name', StringType(), True)]\n",
    "# above statement tell to call an class instance to read age as integer type and name as string type\n",
    "# We enter 'True' to allow it take null values esle will give an error\n",
    "finalstruct=StructType(fields=dataschema)\n",
    "df=spark.read.json(\"people.json\", schema=finalstruct)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see the content of the spark dataframe, use Select statement\n",
    "df.select('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'), Row(age=30, name='Andy')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see top 2 rows\n",
    "df.head(2)\n",
    "# output is list of row objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=None, name='Michael')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see only first row\n",
    "df.head(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to see the contents of multiple columns pass in the name of columns as a list \n",
    "df.select(['age', 'name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+\n",
      "| age|   name|New age|\n",
      "+----+-------+-------+\n",
      "|null|Michael|   null|\n",
      "|  30|   Andy|     30|\n",
      "|  19| Justin|     19|\n",
      "+----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To add a new column\n",
    "df.withColumn('New age', df['age']).show()\n",
    "# pass new name followed by any column whose values you would like to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+\n",
      "| age|   name|Double age|\n",
      "+----+-------+----------+\n",
      "|null|Michael|      null|\n",
      "|  30|   Andy|        60|\n",
      "|  19| Justin|        38|\n",
      "+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Double age', df['age']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|New name|   name|\n",
      "+--------+-------+\n",
      "|    null|Michael|\n",
      "|      30|   Andy|\n",
      "|      19| Justin|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To change the name of existing column\n",
    "df.withColumnRenamed('age', 'New name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To create view \n",
    "df.createOrReplaceTempView('Myview')\n",
    "# spacing not allowed while storing view name \n",
    "results= spark.sql(\"SELECT * FROM Myview\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_result=spark.sql(\"SELECT * FROM Myview WHERE age= 30\")\n",
    "new_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('ops').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inferSchema allows the column to be infered as their respective date type - it automatically gets to knnow if integer or string\n",
    "#etc.\n",
    "df1= spark.read.csv('appl_stock.csv', inferSchema=True, header=True)\n",
    "# header = TRUE treats first coulmn is column headings\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.datetime(2010, 1, 4, 0, 0), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(5)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-19 00:00:00|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-03-05 00:00:00|        214.940006|219.69999500000003|214.62999900000003|218.95000499999998|224905100|28.367064000000003|\n",
      "|2010-03-08 00:00:00|220.01000200000001|        220.090004|        218.250002|        219.079994|107472400|         28.383906|\n",
      "|2010-03-09 00:00:00|218.31000299999997|        224.999996|        217.889994|        223.020004|230064800|28.894371999999997|\n",
      "|2010-03-10 00:00:00|        223.829996|225.48000699999997|223.19999500000003|224.83999300000002|149054500|29.130167999999998|\n",
      "|2010-03-11 00:00:00|        223.909998|        225.500008|        223.319998|        225.500008|101425100|          29.21568|\n",
      "|2010-03-12 00:00:00|         227.37001|227.72999199999998|            225.75|        226.600006|104080900|29.358195000000002|\n",
      "|2010-03-15 00:00:00|225.38000499999998|        225.500008|        220.249994|223.83999599999999|123375700|29.000609000000004|\n",
      "|2010-03-16 00:00:00|        224.180004|        224.979996|        222.510006|224.44999700000002|111727000|         29.079641|\n",
      "|2010-03-17 00:00:00|        224.899994|226.44998900000002|223.26999700000002|224.12000299999997|112739200|         29.036887|\n",
      "|2010-03-18 00:00:00|        224.100002|        224.999996|222.60999500000003|        224.650002| 85527400|29.105553000000004|\n",
      "|2010-03-19 00:00:00|224.79000499999998|        225.240002|221.23000299999998|            222.25|139861400|          28.79461|\n",
      "|2010-03-22 00:00:00|220.46999900000003|        225.999992|        220.150005|        224.750004|114104900|29.118509999999997|\n",
      "|2010-03-23 00:00:00|        225.640011|        228.780003|        224.100002|        228.359993|150607800|         29.586218|\n",
      "|2010-03-24 00:00:00|227.64000299999998|        230.200008|227.50998700000002|229.37000299999997|149445100|         29.717075|\n",
      "|2010-03-25 00:00:00|        230.919998|        230.970013|        226.250011|        226.649994|135571100|         29.364671|\n",
      "|2010-03-26 00:00:00|        228.949993|        231.950008|228.55001099999998|        230.899998|160218800|           29.9153|\n",
      "|2010-03-29 00:00:00|        232.999992|233.86999900000004|        231.619987|        232.389992|135186100|30.108342999999998|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Close>212\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-19 00:00:00|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-03-05 00:00:00|        214.940006|219.69999500000003|214.62999900000003|218.95000499999998|224905100|28.367064000000003|\n",
      "|2010-03-08 00:00:00|220.01000200000001|        220.090004|        218.250002|        219.079994|107472400|         28.383906|\n",
      "|2010-03-09 00:00:00|218.31000299999997|        224.999996|        217.889994|        223.020004|230064800|28.894371999999997|\n",
      "|2010-03-10 00:00:00|        223.829996|225.48000699999997|223.19999500000003|224.83999300000002|149054500|29.130167999999998|\n",
      "|2010-03-11 00:00:00|        223.909998|        225.500008|        223.319998|        225.500008|101425100|          29.21568|\n",
      "|2010-03-12 00:00:00|         227.37001|227.72999199999998|            225.75|        226.600006|104080900|29.358195000000002|\n",
      "|2010-03-15 00:00:00|225.38000499999998|        225.500008|        220.249994|223.83999599999999|123375700|29.000609000000004|\n",
      "|2010-03-16 00:00:00|        224.180004|        224.979996|        222.510006|224.44999700000002|111727000|         29.079641|\n",
      "|2010-03-17 00:00:00|        224.899994|226.44998900000002|223.26999700000002|224.12000299999997|112739200|         29.036887|\n",
      "|2010-03-18 00:00:00|        224.100002|        224.999996|222.60999500000003|        224.650002| 85527400|29.105553000000004|\n",
      "|2010-03-19 00:00:00|224.79000499999998|        225.240002|221.23000299999998|            222.25|139861400|          28.79461|\n",
      "|2010-03-22 00:00:00|220.46999900000003|        225.999992|        220.150005|        224.750004|114104900|29.118509999999997|\n",
      "|2010-03-23 00:00:00|        225.640011|        228.780003|        224.100002|        228.359993|150607800|         29.586218|\n",
      "|2010-03-24 00:00:00|227.64000299999998|        230.200008|227.50998700000002|229.37000299999997|149445100|         29.717075|\n",
      "|2010-03-25 00:00:00|        230.919998|        230.970013|        226.250011|        226.649994|135571100|         29.364671|\n",
      "|2010-03-26 00:00:00|        228.949993|        231.950008|228.55001099999998|        230.899998|160218800|           29.9153|\n",
      "|2010-03-29 00:00:00|        232.999992|233.86999900000004|        231.619987|        232.389992|135186100|30.108342999999998|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1['Close']>212).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|              High|               Low|\n",
      "+------------------+------------------+\n",
      "|        214.499996|212.38000099999996|\n",
      "|        215.589994|        213.249994|\n",
      "|215.18999900000003|        207.240004|\n",
      "|219.69999500000003|214.62999900000003|\n",
      "|        220.090004|        218.250002|\n",
      "|        224.999996|        217.889994|\n",
      "|225.48000699999997|223.19999500000003|\n",
      "|        225.500008|        223.319998|\n",
      "|227.72999199999998|            225.75|\n",
      "|        225.500008|        220.249994|\n",
      "|        224.979996|        222.510006|\n",
      "|226.44998900000002|223.26999700000002|\n",
      "|        224.999996|222.60999500000003|\n",
      "|        225.240002|221.23000299999998|\n",
      "|        225.999992|        220.150005|\n",
      "|        228.780003|        224.100002|\n",
      "|        230.200008|227.50998700000002|\n",
      "|        230.970013|        226.250011|\n",
      "|        231.950008|228.55001099999998|\n",
      "|233.86999900000004|        231.619987|\n",
      "+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to select only few columns based on the filtered data \n",
    "df1.filter(df1['Close']>212).select(['High', 'Low']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-20f45ebb7e7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#some errors we might do\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m212\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m230\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\column.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m         raise ValueError(\"Cannot convert column into bool: please use '&' for 'and', '|' for 'or', \"\n\u001b[0m\u001b[0;32m    683\u001b[0m                          \"'~' for 'not' when building DataFrame boolean expressions.\")\n\u001b[0;32m    684\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
     ]
    }
   ],
   "source": [
    "#some errors we might do \n",
    "df1.filter(df1['Close']>212 and df1['Close']<230).show()\n",
    "# as per the error, we cant use ' and ' ' or ' but rather operators and each syntax in brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|               Low|              High|\n",
      "+------------------+------------------+\n",
      "|212.38000099999996|        214.499996|\n",
      "|        213.249994|        215.589994|\n",
      "|        207.240004|215.18999900000003|\n",
      "|214.62999900000003|219.69999500000003|\n",
      "|        218.250002|        220.090004|\n",
      "|        217.889994|        224.999996|\n",
      "|223.19999500000003|225.48000699999997|\n",
      "|        223.319998|        225.500008|\n",
      "|            225.75|227.72999199999998|\n",
      "|        220.249994|        225.500008|\n",
      "|        222.510006|        224.979996|\n",
      "|223.26999700000002|226.44998900000002|\n",
      "|222.60999500000003|        224.999996|\n",
      "|221.23000299999998|        225.240002|\n",
      "|        220.150005|        225.999992|\n",
      "|        224.100002|        228.780003|\n",
      "|227.50998700000002|        230.200008|\n",
      "|        226.250011|        230.970013|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter((df1['Close']>212) & (df1['Close']<230)).select([\"Low\", \"High\"]).show()\n",
    "# the sequence in which column names are mentioned in select statement, it follows them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+------+---------+\n",
      "|Date|Open|High|Low|Close|Volume|Adj Close|\n",
      "+----+----+----+---+-----+------+---------+\n",
      "+----+----+----+---+-----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result1=df1.filter(df1['Low']==218.25002).show()\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2010, 3, 12, 0, 0), Open=227.37001, High=227.72999199999998, Low=225.75, Close=226.600006, Volume=104080900, Adj Close=29.358195000000002)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get the result as a list (row), use collect() instead of show(). This allows to apply operations on the result later on.\n",
    "result2=df1.filter(df1['Low']==225.75).collect()\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(2010-03-12 00:00:00=1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see particular columns result \n",
    "result2[0](1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': datetime.datetime(2010, 3, 12, 0, 0),\n",
       " 'Open': 227.37001,\n",
       " 'High': 227.72999199999998,\n",
       " 'Low': 225.75,\n",
       " 'Close': 226.600006,\n",
       " 'Volume': 104080900,\n",
       " 'Adj Close': 29.358195000000002}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Better way is to store as dictionary. While storing as dictionary, need to give index of result for storing\n",
    "result2[0].asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "row=result2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227.72999199999998"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.asDict()['High']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
